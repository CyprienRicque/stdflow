[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "stdflow",
    "section": "",
    "text": "Data flow tool that transform your notebooks and python files into pipeline steps by standardizing the data input / output. (for data science projects)\nCreate clean data flow pipelines just by replacing you pd.read_csv() and df.to_csv() by sf.load() and sf.save()."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "stdflow",
    "section": "Install",
    "text": "Install\npip install stdflow"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "stdflow",
    "section": "How to use",
    "text": "How to use\n\nPipelines\n\nfrom stdflow import StepRunner\nfrom stdflow.pipeline import Pipeline\n\n# Pipeline with 2 steps\n\ndm = \"../demo_project/notebooks/\"\n\ningestion_ppl = Pipeline([\n    StepRunner(dm + \"01_ingestion/countries.ipynb\"), \n    StepRunner(dm + \"01_ingestion/world_happiness.ipynb\")\n])\n\n# === OR ===\ningestion_ppl = Pipeline(\n    StepRunner(dm + \"01_ingestion/countries.ipynb\"), \n    StepRunner(dm + \"01_ingestion/world_happiness.ipynb\")\n)\n\n# === OR ===\ningestion_ppl = Pipeline()\ningestion_ppl.add_step(StepRunner(dm + \"01_ingestion/countries.ipynb\"))\n# OR\ningestion_ppl.add_step(dm + \"01_ingestion/world_happiness.ipynb\")\n\n\ningestion_ppl\n\n\n\n================================\n            PIPELINE            \n================================\nSTEP 1\n    path: ../demo_project/notebooks/01_ingestion/countries.ipynb\n    vars: {}\nSTEP 2\n    path: ../demo_project/notebooks/01_ingestion/world_happiness.ipynb\n    vars: {}\n================================\n\n\n\nRun the pipeline\n\ningestion_ppl.run()\n\n=================================================================================\n    01.                ../demo_project/notebooks/01_ingestion/countries.ipynb\n=================================================================================\nVariables: {}\n    Path: countries.ipynb\n    Duration: 0 days 00:00:00.743971\n    Env: {}\nNotebook executed successfully.\n\n\n=================================================================================\n    02.          ../demo_project/notebooks/01_ingestion/world_happiness.ipynb\n=================================================================================\nVariables: {}\n    Path: world_happiness.ipynb\n    Duration: 0 days 00:00:00.643122\n    Env: {}\nNotebook executed successfully."
  },
  {
    "objectID": "index.html#load-and-save-data",
    "href": "index.html#load-and-save-data",
    "title": "stdflow",
    "section": "Load and save data",
    "text": "Load and save data\n\nOption 1: Specify All Parameters\n\nimport stdflow as sf\nimport pandas as pd\n\n# load data from ../demo_project/data/countries/step_loaded/v_202309212245/countries.csv\ndf = sf.load(\n   root=\"../demo_project/data/\",\n   attrs=['countries'],\n   step='loaded',\n   version='202309212245',  # loads v_202309212245\n   file_name='countries.csv',\n   method=pd.read_csv  # or method='csv'\n)\n\n# export data to ./data/raw/twitter/france/step_processed/v_1/countries.csv\nsf.save(\n   df,\n   root=\"../demo_project/data/\",\n   attrs='countries/',\n   step='processed',\n   version='new',  # creates v_new\n   file_name='countries.csv',\n   method=pd.DataFrame.to_csv,  # or method='csv'  or any function that takes the object to export as first input\n)\n\nattrs=countries/::step_name=processed::version=new::file_name=countries.csv\n\n\nEach time you perform a save, a metadata.json file is created in the folder. This keeps track of how your data was created and other information.\n\n\nOption 2: Use default variables\n\nimport stdflow as sf\n\n# use package level default values\nsf.root = \"../demo_project/data/\"\nsf.attrs = 'countries'  # if needed use attrs_in and attrs_out\nsf.step_in = 'loaded'\nsf.step_out = 'processed'\n\ndf = sf.load()\n# ! root / attrs / step : used from default values set above\n# ! version : the last version was automatically used. default: \":last\"\n# ! file_name : the file, alone in the folder, was automatically found\n# ! method : was automatically used from the file extension\n\nsf.save(df)\n# ! root / attrs / step : used from default values set above\n# ! version: used default %Y%m%d%H%M format\n# ! file_name: used from the input (because only one file)\n# ! method : inferred from file name\n\nattrs=countries::step_name=processed::version=202309212303::file_name=countries.csv\n\n\nNote that everything we did at package level can be done with the Step class\n\nfrom stdflow import Step\n\nstep = Step(\n    root=\"../demo_project/data/\",\n    attrs='countries',\n    step_in='loaded',\n    step_out='processed'\n)\n# or set after\nstep.root = \"../demo_project/data/\"\n# ...\n\ndf = step.load(version=':last', file_name=\":auto\", verbose=True)\n\nstep.save(df, verbose=True)\n\nINFO:stdflow.step:Loading data from ../demo_project/data/countries/step_loaded/v_202309212302/countries.csv\nINFO:stdflow.step:Data loaded from ../demo_project/data/countries/step_loaded/v_202309212302/countries.csv\nINFO:stdflow.step:Saving data to ../demo_project/data/countries/step_processed/v_202309212304/countries.csv\nINFO:stdflow.step:Data saved to ../demo_project/data/countries/step_processed/v_202309212304/countries.csv\nINFO:stdflow.step:Saving metadata to ../demo_project/data/countries/step_processed/v_202309212304/\n\n\nattrs=countries::step_name=processed::version=202309212304::file_name=countries.csv"
  },
  {
    "objectID": "index.html#do-not",
    "href": "index.html#do-not",
    "title": "stdflow",
    "section": "Do not",
    "text": "Do not\n\nSave in the same directory from different steps. Because this will erase metadata from the previous step."
  },
  {
    "objectID": "index.html#data-visualization",
    "href": "index.html#data-visualization",
    "title": "stdflow",
    "section": "Data visualization",
    "text": "Data visualization\n\nimport stdflow as sf\n\nstep.save(df, verbose=True, export_viz_tool=True)\n\nINFO:stdflow.step:Saving data to ../demo_project/data/countries/step_processed/v_202309212304/countries.csv\nINFO:stdflow.step:Data saved to ../demo_project/data/countries/step_processed/v_202309212304/countries.csv\nINFO:stdflow.step:Saving metadata to ../demo_project/data/countries/step_processed/v_202309212304/\nINFO:stdflow.step:Exporting viz tool to ../demo_project/data/countries/step_processed/v_202309212304/\n\n\nattrs=countries::step_name=processed::version=202309212304::file_name=countries.csv\n\n\nThis command exports a folder metadata_viz in the same folder as the data you exported. The metadata to display is saved in the metadata.json file.\nIn order to display it you need to get both the file and the folder on your local pc (download if you are working on a server)\nThen go to the html file in your file explorer and open it. it should open in your browser and lets you upload the metadata.json file.\nData flow tool that transform your notebooks and python files into pipeline steps by standardizing the data input / output. (for data science projects)\nCreate clean data flow pipelines just by replacing you pd.read_csv() and df.to_csv() by sf.load() and sf.save()."
  },
  {
    "objectID": "index.html#data-organization",
    "href": "index.html#data-organization",
    "title": "stdflow",
    "section": "Data Organization",
    "text": "Data Organization\n\nFormat\nData folder organization is systematic and used by the function to load and save. If follows this format: root_data_folder/attrs_1/attrs_2/…/attrs_n/step_name/version/file_name\nwhere:\n\nroot_data_folder: is the path to the root of your data folder, and is not exported in the metadata\nattrs: information to classify your dataset (e.g. country, client, …)\nstep_name: name of the step. always starts with step_\nversion: version of the step. always starts with v_\nfile_name: name of the file. can be anything\n\nEach folder is the output of a step. It contains a metadata.json file with information about all files in the folder and how it was generated. It can also contain a html page (if you set html_export=True in save()) that lets you visualize the pipeline and your metadata"
  },
  {
    "objectID": "index.html#tests",
    "href": "index.html#tests",
    "title": "stdflow",
    "section": "Tests",
    "text": "Tests\ntests are run with pytest\n/! run from project root\npytest\nData flow tool that transform your notebooks and python files into pipeline steps by standardizing the data input / output. (for data science projects)\nCreate clean data flow pipelines just by replacing you pd.read_csv() and df.to_csv() by sf.load() and sf.save()."
  },
  {
    "objectID": "step.html",
    "href": "step.html",
    "title": "Step",
    "section": "",
    "text": "source\n\nGStep\n\n GStep (*args, **kwargs)\n\nSingleton Step used at package level\n\nsource\n\n\nStep\n\n Step (root:str|None='./data', attrs:str|list[str]|None=None,\n       file_name:str|None=':auto', method_in:str|object|None=':auto',\n       root_in:str|None=':default',\n       attrs_in:str|list[str]|None=':default', step_in:str|None=None,\n       version_in:str|None=':last', file_name_in:str|None=':default',\n       method_out:str|object|None=':auto', root_out:str|None=':default',\n       attrs_out:str|list[str]|None=':default', step_out:str|None=None,\n       version_out:str|None='%Y%m%d%H%M',\n       file_name_out:str|None=':default',\n       md_all_files:list[FileMetaData]=None,\n       md_direct_input_files:list[FileMetaData]=None)\n\nStep Class for easy data loading and exporting. Also present at package level\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nroot\nstr | None\n./data\nSpecify the root folder of the data. Not exported in metadata\n\n\nattrs\nstr | list[str] | None\nNone\nSpecify the attributes part of the path\n\n\nfile_name\nstr | None\n:auto\nSpecify the file name. See file_name_in and file_name_out for more details on :auto behaviour\n\n\nmethod_in\nstr | object | None\n:auto\nDefault method to load the data. # Method to load the data. Can be a function with path as first argument or a string among [csv, excel, xlsx, xls, parquet, json, pickle, feather, hdf, sql, pkl].\n\n\nroot_in\nstr | None\n:default\nDefault root folder when loading [not recommended, use root instead]\n\n\nattrs_in\nstr | list[str] | None\n:default\nDefault attributes when loading\n\n\nstep_in\nstr | None\nNone\nDefault step name when loading\n\n\nversion_in\nstr | None\n:last\nDefault version name when loading\n\n\nfile_name_in\nstr | None\n:default\nDefault file name when loading\n\n\nmethod_out\nstr | object | None\n:auto\nDefault method to save the data. Can a function with path as first argument or a string among [csv, excel, xlsx, xls, parquet, json, pickle, feather, hdf, sql, pkl]\n\n\nroot_out\nstr | None\n:default\nDefault root folder when saving [not recommended, use root instead]\n\n\nattrs_out\nstr | list[str] | None\n:default\nDefault attributes when saving\n\n\nstep_out\nstr | None\nNone\nDefault step name when saving\n\n\nversion_out\nstr | None\n%Y%m%d%H%M\nDefault version name when saving\n\n\nfile_name_out\nstr | None\n:default\nDefault file name when saving\n\n\nmd_all_files\nlist[FileMetaData]\nNone\nInternal. Do not use\n\n\nmd_direct_input_files\nlist[FileMetaData]\nNone\nInternal. Do not use\n\n\n\n\nsource\n\n\nStep.load\n\n Step.load\n            (root:Union[str,typing_extensions.Literal[':default']]=':defau\n            lt', attrs:Union[list,str,NoneType,typing_extensions.Literal['\n            :default']]=':default', step:Union[str,NoneType,typing_extensi\n            ons.Literal[':default']]=':default', version:Union[str,NoneTyp\n            e,typing_extensions.Literal[':default',':last',':first']]=':de\n            fault', file_name:Union[str,typing_extensions.Literal[':defaul\n            t',':auto']]=':default', method:Union[str,object,typing_extens\n            ions.Literal[':default',':auto']]=':default',\n            alias:str=':ignore', file_glob:bool=False, verbose:bool=False,\n            **kwargs)\n\nLoad data with path such as root/*attrs/step/version/file_name\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nroot\nstr | Literal[‘:default’]\n:default\nRoot folder of the data. Not exported in metadata\n\n\nattrs\nlist | str | None | Literal[‘:default’]\n:default\nAttributes part of the path\n\n\nstep\nstr | None | Literal[‘:default’]\n:default\nStep name, converted to step_{step_name} in the path\n\n\nversion\nstr | None | Literal[(‘:default’, ‘:last’, ‘:first’)]\n:default\nVersion name, converted to v_{version_name} in the path. if :default, uses :last, if :last uses last version based on its name. if :first, uses first version based on its name\n\n\nfile_name\nstr | Literal[(‘:default’, ‘:auto’)]\n:default\nFile name. automatically inferred if there is only one file in the directory\n\n\nmethod\nstr | object | Literal[(‘:default’, ‘:auto’)]\n:default\nMethod to load the data. Can be a function with path as first argument or a string among [csv, excel, xlsx, xls, parquet, json, pickle, feather, hdf, sql, pkl].\n\n\nalias\nstr\n:ignore\nAlias of the dataset to document it and its columns. (feature in development)\n\n\nfile_glob\nbool\nFalse\nIf True, file_name can be a glob pattern\n\n\nverbose\nbool\nFalse\nIf True, print info messages\n\n\nkwargs\n\n\n\n\n\nReturns\nTuple[Any, dict] | Any\n\nLoaded data\n\n\n\n\nsource\n\n\nStep.save\n\n Step.save (data:Union[pandas.core.frame.DataFrame,typing_extensions.Any],\n            root:Union[str,typing_extensions.Literal[':default']]=':defaul\n            t', attrs:Union[list,str,NoneType,typing_extensions.Literal[':\n            default']]=':default', step:Union[str,NoneType,typing_extensio\n            ns.Literal[':default']]=':default', version:Union[str,NoneType\n            ,typing_extensions.Literal[':default'],stdflow.stdflow_types.s\n            trftime_type.Strftime]=':default', file_name:Union[str,typing_\n            extensions.Literal[':default',':auto']]=':default', method:Uni\n            on[str,object,typing_extensions.Literal[':default',':auto']]='\n            :default', alias:str=':ignore', export_viz_tool:bool=False,\n            verbose:bool=False, **kwargs)\n\nSave data with path such as root/attrs/step/version/file_name\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata\npd.DataFrame | Any\n\ndata to save\n\n\nroot\nstr | Literal[‘:default’]\n:default\nRoot folder of the data. Not exported in metadata\n\n\nattrs\nlist | str | None | Literal[‘:default’]\n:default\nAttributes part of the path\n\n\nstep\nstr | None | Literal[‘:default’]\n:default\nStep name, converted to step_{step_name} in the path\n\n\nversion\nstr | None | Literal[‘:default’] | Strftime\n:default\nVersion name, converted to v_{version_name} in the path. by default uses the current date in format %Y%m%d%H%M\n\n\nfile_name\nstr | Literal[(‘:default’, ‘:auto’)]\n:default\nFile name. automatically inferred if there is only one input file\n\n\nmethod\nstr | object | Literal[(‘:default’, ‘:auto’)]\n:default\nMethod to save the data. Can a function with path as first argument or a string among [csv, excel, xlsx, xls, parquet, json, pickle, feather, hdf, sql, pkl]. If function, the first argument must be the path\n\n\nalias\nstr\n:ignore\nAlias of the dataset to document it and its columns. (feature in development)\n\n\nexport_viz_tool\nbool\nFalse\nIf True, export html view of the data and the pipeline it comes from\n\n\nverbose\nbool\nFalse\nIf True, print info messages\n\n\nkwargs\n\n\n\n\n\nReturns\nDataPath\n\nPath object describing where the data is saved\n\n\n\n\nsource\n\n\nStep.var\n\n Step.var (key, value, force=False)\n\nSet a variable which can be overwritten if specified in StepRunner / Pipeline\n\nDocmentTbl(Step.var)"
  },
  {
    "objectID": "step_runner.html",
    "href": "step_runner.html",
    "title": "StepRunner",
    "section": "",
    "text": "source\n\nStepRunner\n\n StepRunner (file_path:str, workspace:str|None=None,\n             function:str|None=None, variables:dict[str,Any]|None=None)\n\nenvironment variables set by stdflow: stdflow__run: if set, the step is executed from a pipeline run stdflow__run__files_path: names of the files executed split by : stdflow__run__ids: ids of the files executed split by : stdflow__run__function_name: name of the function executed stdflow__vars: variables used to run the function"
  },
  {
    "objectID": "pipeline.html",
    "href": "pipeline.html",
    "title": "Pipeline",
    "section": "",
    "text": "source\n\nPipeline\n\n Pipeline (steps:List[StepRunner]|StepRunner=None, *args)\n\nCreate pipeline of notebooks with optional variables\n\nsource\n\n\nPipeline.add_step\n\n Pipeline.add_step (step:Union[stdflow.step_runner.StepRunner,str]=None,\n                    **kwargs)\n\nAdd step to pipeline\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nstep\nStepRunner | str\nNone\nStepRunner or path to notebook\n\n\nkwargs\n\n\n\n\n\n\n\nsource\n\n\nPipeline.verify\n\n Pipeline.verify ()\n\nVerify that all steps are valid\n\nsource\n\n\nPipeline.run\n\n Pipeline.run (progress_bar:bool=False, **kwargs)\n\nRun pipeline\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nprogress_bar\nbool\nFalse\nWhether to show progress bar\n\n\nkwargs\n\n\n\n\n\n\n\nsource\n\n\nPipeline.__call__\n\n Pipeline.__call__ (progress_bar:bool=False, **kwargs)\n\nRun pipeline\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nprogress_bar\nbool\nFalse\nWhether to show progress bar\n\n\nkwargs"
  },
  {
    "objectID": "index.html#best-practices",
    "href": "index.html#best-practices",
    "title": "stdflow",
    "section": "Best Practices:",
    "text": "Best Practices:\n\nDo not use sf.reset as part of your final code\nIn one step, export only to one path (except the version). meaning for one step only one combination of attrs and step_name\nDo not set sub-dirs within the export (i.e. version folder is the last depth). if you need similar operation for different datasets, create pipelines"
  }
]