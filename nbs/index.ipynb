{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stdflow\n",
    "\n",
    "> Data flow tool that transform your notebooks and python files into pipeline steps by standardizing the data input / output. [for Data science project]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data flow tool that transform your notebooks and python files into pipeline steps by standardizing the data input /\n",
    "output. (for data science projects)\n",
    "\n",
    "Create clean data flow pipelines just by replacing you `pd.read_csv()` and `df.to_csv()` by `sf.load()` and `sf.save()`.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "pip install stdflow\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pipelines"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-21T04:36:24.293004Z",
     "start_time": "2023-09-21T04:36:24.289426Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "\u001B[1m\n================================\n            PIPELINE            \n================================\n\n\u001B[0m\u001B[1mSTEP 1\u001B[0m\n\tpath: ../demo_project/notebooks/01_ingestion/countries.ipynb\n\tvars: {}\n\n\u001B[1mSTEP 2\u001B[0m\n\tpath: ../demo_project/notebooks/01_ingestion/world_happiness.ipynb\n\tvars: {}\n\n\u001B[1m================================\u001B[0m"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stdflow import StepRunner\n",
    "from stdflow.pipeline import Pipeline\n",
    "\n",
    "# Pipeline with 2 steps\n",
    "\n",
    "dm = \"../demo_project/notebooks/\"\n",
    "\n",
    "ingestion_ppl = Pipeline([\n",
    "    StepRunner(dm + \"01_ingestion/countries.ipynb\"), \n",
    "    StepRunner(dm + \"01_ingestion/world_happiness.ipynb\")\n",
    "])\n",
    "\n",
    "# === OR ===\n",
    "ingestion_ppl = Pipeline(\n",
    "    StepRunner(dm + \"01_ingestion/countries.ipynb\"), \n",
    "    StepRunner(dm + \"01_ingestion/world_happiness.ipynb\")\n",
    ")\n",
    "\n",
    "# === OR ===\n",
    "ingestion_ppl = Pipeline()\n",
    "ingestion_ppl.add_step(StepRunner(dm + \"01_ingestion/countries.ipynb\"))\n",
    "# OR\n",
    "ingestion_ppl.add_step(dm + \"01_ingestion/world_happiness.ipynb\")\n",
    "\n",
    "\n",
    "ingestion_ppl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run the pipeline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:stdflow.environ_manager:setting variables {}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m===============================\u001B[0m\n",
      "\u001B[1m    61.../demo_project/notebooks/01_ingestion/countries.ipynb\u001B[0m\n",
      "\u001B[1m===============================\u001B[0m\u001B[0m\n",
      "Variables: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:stdflow.environ_manager:setting variables {}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tPath: countries.ipynb\n",
      "\tDuration: 0 days 00:00:00.771219\n",
      "\tEnv: {}\n",
      "\u001B[1m\u001B[32mNotebook executed successfully.\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m===============================\u001B[0m\n",
      "\u001B[1m    61.../demo_project/notebooks/01_ingestion/world_happiness.ipynb\u001B[0m\n",
      "\u001B[1m===============================\u001B[0m\u001B[0m\n",
      "Variables: {}\n",
      "\tPath: world_happiness.ipynb\n",
      "\tDuration: 0 days 00:00:00.644832\n",
      "\tEnv: {}\n",
      "\u001B[1m\u001B[32mNotebook executed successfully.\u001B[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ingestion_ppl.run()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T04:36:29.232853Z",
     "start_time": "2023-09-21T04:36:25.051036Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load and save data\n",
    "**Specify everything**\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import stdflow as sf\n",
    "import pandas as pd\n",
    "\n",
    "# load data from ./data/raw/twitter/france/step_raw/v_1/countries of the world.csv\n",
    "df = sf.load(\n",
    "   root=\"./data\", \n",
    "   attrs=['twitter', 'france'], # or attrs='twitter/france'\n",
    "   step='raw', \n",
    "   version='1', \n",
    "   file_name='countries of the world.csv',\n",
    "   method=pd.read_csv  # or method='csv'\n",
    ")\n",
    "\n",
    "# export data to ./data/raw/twitter/france/step_processed/v_1/countries.csv\n",
    "sf.save(\n",
    "   df, \n",
    "   root=\"./data\", \n",
    "   attrs=['twitter', 'france'], \n",
    "   step='processed', \n",
    "   version='1', \n",
    "   file_name='countries.csv', \n",
    "   method=pd.to_csv  # or method='csv'  or any function that takes the object to export as first input \n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Each time you perform a save, a metadata.json file is created in the folder.\n",
    "This keeps track of how your data was created and other information.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "**More Convenient Method**\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import stdflow as sf\n",
    "\n",
    "# use package level default values\n",
    "sf.root = \"./data\"\n",
    "sf.attrs = ['twitter', 'france']  # if needed use attrs_in and attrs_out\n",
    "sf.step_in = 'raw'\n",
    "sf.step_out = 'processed'\n",
    "\n",
    "df = sf.load()  \n",
    "# ! root / attrs / step : used from default values set above\n",
    "# ! version : the last version was automatically used. default: \":last\"\n",
    "# ! file_name : the file, alone in the folder, was automatically found\n",
    "# ! method : was automatically used from the file extension\n",
    "\n",
    "sf.save(df)\n",
    "# ! root / attrs / step : used from default values set above\n",
    "# ! version: used default %Y%m%d%H%M format\n",
    "# ! file_name: used from the input (because only one file)\n",
    "# ! method : inferred from file name\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Note that everything we did at package level can be done with the Step class\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from stdflow import Step\n",
    "\n",
    "step = Step(root=\"./data\", attrs=['twitter', 'france'], step_in='raw', step_out='processed')\n",
    "# or set after\n",
    "step.root = \"./data\"\n",
    "# ...\n",
    "\n",
    "df = step.load(version=':last', file_name=\":auto\", verbose=True)\n",
    "\n",
    "step.save(df, verbose=True)\n",
    "`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Do not\n",
    "\n",
    "- Save in the same directory from different steps. Because this will erase metadata from the previous step.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Data visualization\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import stdflow as sf\n",
    "sf.save({'what?': \"very cool data\"},..., export_viz_tool=True) # exports viz folder\n",
    "`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This command exports a folder `metadata_viz` in the same folder as the data you exported.\n",
    "The metadata to display is saved in the metadata.json file.\n",
    "\n",
    "In order to display it you need to get both the file and the folder on your local pc (download if you are working on a server)\n",
    "\n",
    "Then go to the html file in your file explorer and open it. it should open in your browser and lets you upload the metadata.json file.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data flow tool that transform your notebooks and python files into pipeline steps by standardizing the data input /\n",
    "output. (for data science projects)\n",
    "\n",
    "Create clean data flow pipelines just by replacing you `pd.read_csv()` and `df.to_csv()` by `sf.load()` and `sf.save()`.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Data Organization\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Format\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data folder organization is systematic and used by the function to load and save.\n",
    "If follows this format:\n",
    "root_data_folder/attrs_1/attrs_2/.../attrs_n/step_name/version/file_name\n",
    "\n",
    "where:\n",
    "\n",
    "- root_data_folder: is the path to the root of your data folder, and is not exported in the metadata\n",
    "- attrs: information to classify your dataset (e.g. country, client, ...)\n",
    "- step_name: name of the step. always starts with `step_`\n",
    "- version: version of the step. always starts with `v_`\n",
    "- file_name: name of the file. can be anything\n",
    "\n",
    "Each folder is the output of a step. It contains a metadata.json file with information about all files in the folder\n",
    "and how it was generated.\n",
    "It can also contain a html page (if you set `html_export=True` in `save()`) that lets you visualize the pipeline and your metadata\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Pipeline\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A pipeline is composed of steps\n",
    "each step should export the data by using export_tabular_data function which does the export in a standard way\n",
    "a step can be\n",
    "\n",
    "- a file: jupyter notebook\n",
    "- python file (in coming)\n",
    "- a python function (in coming)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Recommended steps\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can set up any step you want. However, just like any tools there are good/bad and common ways to use it.\n",
    "\n",
    "The recommended way to use it is:\n",
    "\n",
    "1. Load\n",
    "    - Use a custom load function to load you raw datasets if needed\n",
    "    - Fix column names\n",
    "    - Fix values\n",
    "        - Except those for which you would like to test multiple methods that impacts ml models.\n",
    "    - Fix column types\n",
    "2. Merge\n",
    "    - Merge data from multiple sources\n",
    "3. Transform\n",
    "    - Pre-processing step along with most plots and analysis\n",
    "4. Feature engineering (step that is likely to see many iterations)\n",
    "   > *The output of this step goes into the model*\n",
    "    - Create features\n",
    "    - Fill missing values\n",
    "5. Model\n",
    "    - This step likely contains gridsearch and therefore output multiple resulting datasets\n",
    "    - Train model\n",
    "    - Evaluate model (or moved to a separate step)\n",
    "    - Save model\n",
    "\n",
    "**Best Practices**:\n",
    "- Do not use ```sf.reset``` as part of your final code\n",
    "- In one step, export only to one path (except the version). meaning for one step only one combination of attrs and step_name\n",
    "- Do not set sub-dirs within the export (i.e. version folder is the last depth). if you need similar operation\n",
    "  for different datasets, create pipelines\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Tests\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "tests are run with pytest\n",
    "\n",
    "/!\\ run from project root\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "pytest\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data flow tool that transform your notebooks and python files into pipeline steps by standardizing the data input /\n",
    "output. (for data science projects)\n",
    "\n",
    "Create clean data flow pipelines just by replacing you `pd.read_csv()` and `df.to_csv()` by `sf.load()` and `sf.save()`.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
